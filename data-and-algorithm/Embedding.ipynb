{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import downloader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    " \n",
    "num_topics = 50\n",
    "model_bert = SentenceTransformer('bert-base-nli-max-tokens')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv(\"./raw_data/Books.csv\")\n",
    "df_summary = pd.read_csv('booksummaries.txt', sep = '\\t')\n",
    "df_summary.columns = ['Code1', 'Code2', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Code3', 'Book-Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_ISBN_to_Title(df_merged):\n",
    "    ISBN_to_Title = {}\n",
    "    for row in df_merged.index:\n",
    "        title = df_merged[\"Book-Title\"][row]\n",
    "        isbn = df_merged[\"ISBN\"][row]\n",
    "        ISBN_to_Title[isbn] = title\n",
    "    return ISBN_to_Title\n",
    "\n",
    "def merge_book_and_summary(df_summary, df_books):\n",
    "    df_merged = pd.merge(df_summary, df_books, how='inner', on =['Book-Title'])\n",
    "    df_merged = df_merged.drop(['Code1','Code2','Code3','Publisher', 'Year-Of-Publication_x', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'Book-Author_y', 'Year-Of-Publication_y'], axis=1)\n",
    "    df_merged = df_merged[df_merged['Book-Title'] != 'Deathstalker Rebellion']\n",
    "    return df_merged\n",
    "\n",
    "df_merged = merge_book_and_summary(df_summary, df_books).drop_duplicates(subset=['Book-Title'])\n",
    "#df_merged.to_csv('Books_Summary_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_books_to_vecs(books):\n",
    "  embedding_bert = np.array(model_bert.encode(books))\n",
    "  return embedding_bert\n",
    "\n",
    "def reduce_tsne(embedding):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    reduced = tsne.fit_transform( embedding )\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "def predict_topics_with_kmeans(embeddings,num_topics):\n",
    "  kmeans_model = KMeans(num_topics)\n",
    "  kmeans_model.fit(embeddings)\n",
    "  topics_labels = kmeans_model.predict(embeddings)\n",
    "  return topics_labels\n",
    "\n",
    "df_summary_bert = df_merged\n",
    "all_summary_list = df_summary_bert[\"Book-Summary\"].tolist()\n",
    "all_summary_embeddings = all_books_to_vecs(all_summary_list)\n",
    "df_summary[\"vector\"] = all_summary_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize\n",
    "def plot_embeddings(embedding, labels,title):\n",
    "\n",
    "    labels = np.array( labels )\n",
    "    distinct_labels =  set( labels )\n",
    "    \n",
    "    n = len(embedding)\n",
    "    counter = Counter(labels)\n",
    "    for i in range(len( distinct_labels )):\n",
    "        ratio = (counter[i] / n )* 100\n",
    "        cluster_label = f\"cluster {i}: { round(ratio,2)}\"\n",
    "        x = embedding[:, 0][labels == i]\n",
    "        y = embedding[:, 1][labels == i]\n",
    "        plt.plot(x, y, '.', alpha=0.4, label= cluster_label)\n",
    "    plt.legend(title=\"Topic\",loc = 'upper left', bbox_to_anchor=(1.01,1))\n",
    "    plt.title(title)\n",
    "\n",
    "embedding_bert_tsne =  reduce_tsne(all_summary_embeddings)\n",
    "labels_bert_tsne  = predict_topics_with_kmeans(embedding_bert_tsne,num_topics)\n",
    "df_summary_bert[\"group\"] = labels_bert_tsne\n",
    "df_summary_bert[\"vector\"] = all_summary_embeddings.tolist()\n",
    "#plot_embeddings(embedding_bert_tsne,labels_bert_tsne,\"Bert with T-sne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Book2Vec(df_merged):\n",
    "    title_dict = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for row in df_merged.index:\n",
    "        title = df_merged['Book-Title'][row]\n",
    "        summary = df_merged['Book-Summary'][row]\n",
    "        summary_nonum = re.sub(r'\\d+', '', summary) #remove number\n",
    "        tokenizer = RegexpTokenizer(r'\\w+') #remove punctuation\n",
    "        summary_tokens = tokenizer.tokenize(summary_nonum) \n",
    "        filtered_summary_tokens = [w for w in summary_tokens if not w.lower() in stop_words] #remove stop words\n",
    "\n",
    "        summary_vectors = np.zeros(300)\n",
    "    \n",
    "        n = 0\n",
    "        for summary_token in filtered_summary_tokens:\n",
    "            try:\n",
    "                summary_token_vec = glove_vectors[summary_token]\n",
    "            except:\n",
    "                summary_token_vec = np.zeros(300)\n",
    "            summary_vectors += summary_token_vec\n",
    "            n += 1\n",
    "        summary_list.append (summary_vectors / n)\n",
    "    \n",
    "    return summary_list\n",
    "\n",
    "summary_list = compute_Book2Vec(df_summary_bert)\n",
    "df_summary = df_summary.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1)\n",
    "df_summary = df_summary.rename(columns={\"vector\": \"BERT\"})\n",
    "df_summary[\"word2vec\"] = summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv(\"book-vec-group.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
